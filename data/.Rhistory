X.list[[2]] = sweep(X.list[[2]], 2, X1.mean)
X.list[[2]] = sweep(X.list[[2]], 2, X1.sd, "/")
apply(X.list[[2]], 2, sd)
Y.list = lapply(1:2, function(x) Y[id[[x]]]) # train,test
X.list = lapply(1:2, function(x) X[id[[x]],])
label.list = lapply(1:2, function(x) label[id[[x]]])
# ----------------------------------- do scale ----------------------------------
X1.mean = apply(X.list[[1]], 2, mean)
X1.sd = apply(X.list[[1]], 2, sd)
X.list[[1]] = sweep(X.list[[1]], 2, X1.mean)
X.list[[1]] = sweep(X.list[[1]], 2, X1.sd, "/")
X.list[[2]] = sweep(X.list[[2]], 2, X1.mean)
X.list[[2]] = sweep(X.list[[2]], 2, X1.sd, "/")
X.list
# ----------------------------- Calculating interaction term ------------------------------
X.interaction.list = list()
X.plus.inter.list = list()
for (m in 1:2){
X.interaction.list[[m]] = list()
k = 0
for (i in 1:p){
for (j in 1:p){
if (i < j){
k = k+1
X.interaction.list[[m]][[k]] = X.list[[m]][,i]*X.list[[m]][,j]
}
}
}
X.interaction.list[[m]] = do.call(cbind, X.interaction.list[[m]])
#  X.plus.inter.list[[m]] = cbind(X.list[[m]], X.interaction.list[[m]])
}
X1.interaction.mean = apply(X.interaction.list[[1]], 2, mean)
X1.interaction.sd = apply(X.interaction.list[[1]], 2, sd)
X1.interaction.sd
X1.interaction.mean
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.mean)
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.sd, "/")
X.interaction.list
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.mean)
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.sd, "/")
apply(X.interaction.list[[1]], 2, mean)
apply(X.interaction.list[[1]], 2, sd)
apply(X.interaction.list[[2]], 2, sd)
# ----------------------------- Calculating interaction term ------------------------------
X.interaction.list = list()
X.plus.inter.list = list()
for (m in 1:2){
X.interaction.list[[m]] = list()
k = 0
for (i in 1:p){
for (j in 1:p){
if (i < j){
k = k+1
X.interaction.list[[m]][[k]] = X.list[[m]][,i]*X.list[[m]][,j]
}
}
}
X.interaction.list[[m]] = do.call(cbind, X.interaction.list[[m]])
#  X.plus.inter.list[[m]] = cbind(X.list[[m]], X.interaction.list[[m]])
}
X1.interaction.mean = apply(X.interaction.list[[1]], 2, mean)
X1.interaction.sd = apply(X.interaction.list[[1]], 2, sd)
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.mean)
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.sd, "/")
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.mean)
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.sd, "/")
apply(X.interaction.list[[2]], 2, sd)
apply(X.interaction.list[[2]], 2, mean)
# if the std is really small just subtract the mean in the following step
# X1.interaction.sd = sapply(X1.interaction.sd, function(x) ifelse(x<1e-5, 1, x))
# X.interaction.list[[1]] = t(apply(X.interaction.list[[1]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
# X.interaction.list[[2]] = t(apply(X.interaction.list[[2]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
X.plus.inter.list = lapply(1:2, function(x) cbind(X.list[[x]], X.interaction.list[[x]]))
# --- computing distance correlation to select favorite number of features including interaction features ---
# number of features to be selected
p_dc = 200
cl = makeCluster(4) # number of cores you can use
registerDoParallel(cl)
dc.vec = foreach(col_ix = 1:ncol(X.plus.inter.list[[1]]), .packages = "energy", .combine = "c") %dopar% {
dcor(Y.list[[1]], X.plus.inter.list[[1]][,col_ix])
}
stopCluster(cl)
X.selected.feature.id = order(dc.vec, decreasing = TRUE)[1:p_dc]
X.selected.feature.id
X.selected.feature.list = lapply(1:2, function(ix) X.plus.inter.list[[ix]][, X.selected.feature.id])
# add to the last... probably to fix a random forest bug
feature.ncol = ncol(X.selected.feature.list[[1]])
colnames(X.selected.feature.list[[1]]) = as.character(seq(1, feature.ncol))
colnames(X.selected.feature.list[[2]]) = as.character(seq(1, feature.ncol))
feature.ncol
# ----------------------------------------- main -------------------------------------------
# ------------- parameters --------------
nfolds = 5
alpha0 = 0
gamma.vec = exp(rev(seq(-2, 7, length.out = 50)))
# initial point for optimization, first K-1 parameters are thetas, then the first coming p are coefficients the last p are slack variable
initial.x = c(seq(-2, 2, length.out = length(levels(label.list[[1]]))-1), rep(0,p_dc), rep(1,p_dc))
lDi = 20
ordinlog.list = cv.ordinlog.both.noDb(label.list[[1]], X.selected.feature.list[[1]], Y.list[[1]], gamma.vec, alpha0, initial.x, nfolds, lDi)
ordin.ml = ordinlog.list$ordin.ml
sl.list = lapply(1:2, function(x) as.vector(X.selected.feature.list[[x]]%*%ordin.ml$w))
# delete outliers
sl.list = lapply(1:2, function(ix){
sl.list[[ix]][sl.list[[ix]] > boxplot(sl.list[[ix]], plot = F)$stats[5,1]] = boxplot(sl.list[[ix]], plot = F)$stats[5,1]
sl.list[[ix]][sl.list[[ix]] < boxplot(sl.list[[ix]], plot = F)$stats[1,1]] = boxplot(sl.list[[ix]], plot = F)$stats[1,1]
sl.list[[ix]]
})
Di.vec = seq(sd(sl.list[[1]])/5, sd(sl.list[[1]])*2, length.out = lDi)
Di.selected = Di.vec[ordinlog.list$Di.id]
id.which = ordinlog.list$id
if(id.which == 1){
ml.rf = randomForest(x = X.selected.feature.list[[1]], y = Y.list[[1]], keep.inbag = T, ntree = 100)
wrf.list = rf.weight(ml.rf, X.selected.feature.list[[1]], X.selected.feature.list[[2]])
mymethod.res = SlRf.weight.noDb(wrf.list, Y.list[[1]], sl.list[[1]], sl.list[[2]], Di.selected)
}else{
mymethod.res = slnp.noDb(X.selected.feature.list[[1]], Y.list[[1]], sl.list[[1]], X.selected.feature.list[[2]], Y.list[[2]], sl.list[[2]], Di.selected)
}
print("Finish local fitting without penalization")
Yhat.mymethod = mymethod.res$Yhat
Yhat.mymethod = exp(Yhat.mymethod)
mae.mymethod = mean(abs(Yhat.mymethod - exp(Y.list[[2]])))
gamma.vec
lDi
corr.mymethod = cor(Yhat.mymethod, exp(Y.list[[2]]))
Y.list = lapply(1:2, function(x) Y[id[[x]]]) # train,test
X.list = lapply(1:2, function(x) X[id[[x]],])
label.list = lapply(1:2, function(x) label[id[[x]]])
# ----------------------------------- do scale ----------------------------------
X1.mean = apply(X.list[[1]], 2, mean)
X1.sd = apply(X.list[[1]], 2, sd)
X.list[[1]] = sweep(X.list[[1]], 2, X1.mean)
X.list[[1]] = sweep(X.list[[1]], 2, X1.sd, "/")
X.list[[2]] = sweep(X.list[[2]], 2, X1.mean)
X.list[[2]] = sweep(X.list[[2]], 2, X1.sd, "/")
# ----------------------------- Calculating interaction term ------------------------------
X.interaction.list = list()
X.plus.inter.list = list()
for (m in 1:2){
X.interaction.list[[m]] = list()
k = 0
for (i in 1:p){
for (j in 1:p){
if (i < j){
k = k+1
X.interaction.list[[m]][[k]] = X.list[[m]][,i]*X.list[[m]][,j]
}
}
}
X.interaction.list[[m]] = do.call(cbind, X.interaction.list[[m]])
#  X.plus.inter.list[[m]] = cbind(X.list[[m]], X.interaction.list[[m]])
}
X1.interaction.mean = apply(X.interaction.list[[1]], 2, mean)
X1.interaction.sd = apply(X.interaction.list[[1]], 2, sd)
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.mean)
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.sd, "/")
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.mean)
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.sd, "/")
# # if the std is really small just subtract the mean in the following step
# X1.interaction.sd = sapply(X1.interaction.sd, function(x) ifelse(x<1e-5, 1, x))
# X.interaction.list[[1]] = t(apply(X.interaction.list[[1]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
# X.interaction.list[[2]] = t(apply(X.interaction.list[[2]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
X.plus.inter.list = lapply(1:2, function(x) cbind(X.list[[x]], X.interaction.list[[x]]))
# --- computing distance correlation to select favorite number of features including interaction features ---
# number of features to be selected
p_dc = 200
cl = makeCluster(4) # number of cores you can use
registerDoParallel(cl)
dc.vec = foreach(col_ix = 1:ncol(X.plus.inter.list[[1]]), .packages = "energy", .combine = "c") %dopar% {
dcor(Y.list[[1]], X.plus.inter.list[[1]][,col_ix])
}
stopCluster(cl)
# dc.vec = apply(X.plus.inter.list[[1]], 2, function(feature) dcor(Y.list[[1]], feature))
print("Finish calculation distance correlation")
X.selected.feature.id = order(dc.vec, decreasing = TRUE)[1:p_dc]
X.selected.feature.list = lapply(1:2, function(ix) X.plus.inter.list[[ix]][, X.selected.feature.id])
write.table(X.selected.feature.id, paste0("feature_id+", name.id, ".txt"), sep="\t", row.names = F, col.names = F, append = T)
print("Finish addig top interaction terms into design matrix")
# add to the last... probably to fix a random forest bug
feature.ncol = ncol(X.selected.feature.list[[1]])
colnames(X.selected.feature.list[[1]]) = as.character(seq(1, feature.ncol))
colnames(X.selected.feature.list[[2]]) = as.character(seq(1, feature.ncol))
# ----------------------------------------- main -------------------------------------------
# ------------- parameters --------------
nfolds.log = 5 # ordinal logistic: Sl
nfolds.llr = 5 # local linear regression
alpha0 = 0
gamma.vec = exp(rev(seq(-2, 7, length.out = 50)))
# initial point for optimization, first K-1 parameters are thetas, then the first coming p are coefficients the last p are slack variable
initial.x = c(seq(-2, 2, length.out = length(levels(label.list[[1]]))-1), rep(0,p_dc), rep(1,p_dc))
measure.type = "corr"
# ------------------------ ordinal logistic: Sl --------------------------
ordinlog.list = cv.ordinlog.en(label.list[[1]], X.selected.feature.list[[1]], Y.list[[1]], gamma.vec, alpha0, initial.x, nfolds.log, "corr")
ordin.ml = ordinlog.list$ordin.ml
sl.list = lapply(1:2, function(x) as.vector(X.selected.feature.list[[x]]%*%ordin.ml$w))
# delete outliers
sl.list = lapply(1:2, function(ix){
sl.list[[ix]][sl.list[[ix]] > boxplot(sl.list[[ix]], plot = F)$stats[5,1]] = boxplot(sl.list[[ix]], plot = F)$stats[5,1]
sl.list[[ix]][sl.list[[ix]] < boxplot(sl.list[[ix]], plot = F)$stats[1,1]] = boxplot(sl.list[[ix]], plot = F)$stats[1,1]
sl.list[[ix]]
})
# tuning parameter Di for SlRf
Di.vec = seq(sd(sl.list[[1]])/5, sd(sl.list[[1]])*2, length.out = 20)
# ----------------------------- SlRf LWPR ---------------------------------
par.list = cv.bothPen.noDb.nolambda(label.list[[1]], X.selected.feature.list[[1]], Y.list[[1]], alpha, nfolds.llr, sl.list[[1]], Di.vec)
par.list
Di.selected = par.list$Di
lam.id = par.list$lambda.id
id.which = par.list$id.which
if(id.which == 1){
ml.rf = randomForest(x = X.selected.feature.list[[1]], y = Y.list[[1]], keep.inbag = T, ntree = 100)
wrf.list = rf.weight(ml.rf, X.selected.feature.list[[1]], X.selected.feature.list[[2]])
mymethod.res = SlRf.weight.noDb(wrf.list, Y.list[[1]], sl.list[[1]], sl.list[[2]], Di.selected)
}else{
mymethod.res = slnp.noDb(X.selected.feature.list[[1]], Y.list[[1]], sl.list[[1]], X.selected.feature.list[[2]], Y.list[[2]], sl.list[[2]], Di.selected)
}
print("Finish local fitting without penalization")
Yhat.mymethod = mymethod.res$Yhat
rwrf.list = mymethod.res$w.list
Yhat.mymethodPen = predict.penalized.origin.method.nolambda(X.selected.feature.list[[1]], Y.list[[1]], X.selected.feature.list[[2]], rwrf.list, lam.id, alpha)
# ------------------------------ Caculating results ----------------------------------------
# -------------------- mae and corr --------------------------
Yhat.mymethod = exp(Yhat.mymethod)
Yhat.mymethodPen = exp(Yhat.mymethodPen)
mae.mymethod = mean(abs(Yhat.mymethod - exp(Y.list[[2]])))
corr.mymethod = cor(Yhat.mymethod, exp(Y.list[[2]]))
mae.mymethodPen = mean(abs(Yhat.mymethodPen - exp(Y.list[[2]])))
corr.mymethodPen = cor(Yhat.mymethodPen, exp(Y.list[[2]]))
mae.mymethod
mae.mymethodPen
predict.penalized.origin.method.nolambda = function(X.train, Y.train, X.test, wrf.list, lam.id, alpha){
n.test = dim(X.test)[1]
p = dim(X.test)[2]
nlambda = 100
#diff.mtx.list = diff.matrix(X.train, X.test)
diff.mtx.list = diff.matrix.weight.standardize(X.train, X.test, wrf.list)
beta = c()
for (x in 1:n.test){
if(sd(Y.train[wrf.list[[x]]>1e-5])==0 | length(Y.train[wrf.list[[x]]>1e-5])<=10){
beta[x] = sum(Y.train*wrf.list[[x]])
}
else{
lam_max = lambda.max(diff.mtx.list[[x]], Y.train, wrf.list[[x]], alpha)
lambda.vec = exp(seq(log(lam_max), log(lam_max*1e-3), length.out = nlambda))
fit = glmnet(x = diff.mtx.list[[x]], y = Y.train, weights = wrf.list[[x]], alpha = alpha, lambda = lambda.vec, standardize = F)
# print(lambda.vec[lam.id])
beta.x = as.vector(coef(fit, s=lambda.vec[lam.id]))
print(beta.x)
beta[x] = beta.x[1]}
}
return(beta)
}
Yhat.mymethodPen = predict.penalized.origin.method.nolambda(X.selected.feature.list[[1]], Y.list[[1]], X.selected.feature.list[[2]], rwrf.list, lam.id, alpha)
source('~/GitHub/LWPR/function/fun.noDb.R', echo=TRUE)
source('~/GitHub/LWPR/function/ordinlog1.R', echo=TRUE)
source('~/GitHub/LWPR/function/fun.rfguided.R', echo=TRUE)
# ----------------------------------------- main -------------------------------------------
# ------------- parameters --------------
nfolds = 5
alpha0 = 0
gamma.vec = exp(rev(seq(-2, 7, length.out = 50)))
# initial point for optimization, first K-1 parameters are thetas, then the first coming p are coefficients the last p are slack variable
initial.x = c(seq(-2, 2, length.out = length(levels(label.list[[1]]))-1), rep(0,p_dc), rep(1,p_dc))
lDi = 20
ordinlog.list = cv.ordinlog.both.noDb(label.list[[1]], X.selected.feature.list[[1]], Y.list[[1]], gamma.vec, alpha0, initial.x, nfolds, lDi)
ordin.ml = ordinlog.list$ordin.ml
sl.list = lapply(1:2, function(x) as.vector(X.selected.feature.list[[x]]%*%ordin.ml$w))
# delete outliers
sl.list = lapply(1:2, function(ix){
sl.list[[ix]][sl.list[[ix]] > boxplot(sl.list[[ix]], plot = F)$stats[5,1]] = boxplot(sl.list[[ix]], plot = F)$stats[5,1]
sl.list[[ix]][sl.list[[ix]] < boxplot(sl.list[[ix]], plot = F)$stats[1,1]] = boxplot(sl.list[[ix]], plot = F)$stats[1,1]
sl.list[[ix]]
})
Di.vec = seq(sd(sl.list[[1]])/5, sd(sl.list[[1]])*2, length.out = lDi)
Di.selected = Di.vec[ordinlog.list$Di.id]
id.which = ordinlog.list$id
if(id.which == 1){
ml.rf = randomForest(x = X.selected.feature.list[[1]], y = Y.list[[1]], keep.inbag = T, ntree = 100)
wrf.list = rf.weight(ml.rf, X.selected.feature.list[[1]], X.selected.feature.list[[2]])
mymethod.res = SlRf.weight.noDb(wrf.list, Y.list[[1]], sl.list[[1]], sl.list[[2]], Di.selected)
}else{
mymethod.res = slnp.noDb(X.selected.feature.list[[1]], Y.list[[1]], sl.list[[1]], X.selected.feature.list[[2]], Y.list[[2]], sl.list[[2]], Di.selected)
}
print("Finish local fitting without penalization")
Yhat.mymethod = mymethod.res$Yhat
Yhat.mymethod = exp(Yhat.mymethod)
mae.mymethod = mean(abs(Yhat.mymethod - exp(Y.list[[2]])))
a = 0.5
t = 1
library(readr)
setwd("C:/Users/peiyao/Documents/GitHub/LWPR/data")
X0 = as.matrix(read_table("X1a.txt", col_names = F))
Y0 = as.matrix(read_table("Y5T.txt", col_names = F))[, 4+5*(t-1)]
label0 = as.ordered(read_table("label1.txt", col_names = F)$X1)
# remove NAs in Y
id.cut1 = !is.na(Y0)
X0 = X0[id.cut1, ]
Y0 = Y0[id.cut1]
label0 = label0[id.cut1]
# remove negatives in Y
id.cut2 = (Y0 > 0)
X0 = X0[id.cut2,]
Y0 = Y0[id.cut2]
label0 = label0[id.cut2]
# only select MRI+PET
Xs = X0[, 1:186]
# within group median , each element from the list is the median within the group
# list of length 3, each element 1 by 186 group median of each feature
Xs.med.list = lapply(levels(label0), function(x) apply(Xs[label0 == x, ], 2, function(y) median(y, na.rm = T)))
# impute with median
Xs.missing = is.na(Xs)
for (i in 1:nrow(Xs)){
ip.med = Xs.med.list[[as.numeric(label0[i])]]
Xs[i, Xs.missing[i, ]] = ip.med[Xs.missing[i, ]]
}
X = Xs
Y = Y0
label = label0
library(caret)
library(methods) # "is function issue by Rscript"
library(energy)
library(glmnet)
library(randomForest)
library(foreach)
library(doParallel)
source('~/GitHub/LWPR/function/fun.rfguided.R', echo=TRUE)
source('~/GitHub/LWPR/function/ordinlog1.R', echo=TRUE)
source('~/GitHub/LWPR/function/fun.noDb.R', echo=TRUE)
name.id = paste0("t=", as.character(t), "+alpha=", as.character(alpha))
# ---------------------- creating training and testing set -----------------------------
n = dim(X)[1]
p = dim(X)[2]
myseed = 551
idtrain = unlist(createDataPartition(label, times = 1, p = 3/4))
idtest = (1:n)[-idtrain]
Y = log(Y+1)
id = list(idtrain, idtest)
Y.list = lapply(1:2, function(x) Y[id[[x]]]) # train,test
X.list = lapply(1:2, function(x) X[id[[x]],])
label.list = lapply(1:2, function(x) label[id[[x]]])
# ----------------------------------- do scale ----------------------------------
X1.mean = apply(X.list[[1]], 2, mean)
X1.sd = apply(X.list[[1]], 2, sd)
X.list[[1]] = sweep(X.list[[1]], 2, X1.mean)
X.list[[1]] = sweep(X.list[[1]], 2, X1.sd, "/")
X.list[[2]] = sweep(X.list[[2]], 2, X1.mean)
X.list[[2]] = sweep(X.list[[2]], 2, X1.sd, "/")
# # if the std is really small just subtract the mean in the following step
# X1.sd = sapply(X1.sd, function(x) ifelse(x<1e-5, 1, x))
# X.list[[1]] = t(apply(X.list[[1]], 1, function(x) (x - X1.mean)/X1.sd))
# X.list[[2]] = t(apply(X.list[[2]], 1, function(x) (x - X1.mean)/X1.sd))
print("Finish scaling features")
# ----------------------------- Calculating interaction term ------------------------------
X.interaction.list = list()
X.plus.inter.list = list()
for (m in 1:2){
X.interaction.list[[m]] = list()
k = 0
for (i in 1:p){
for (j in 1:p){
if (i < j){
k = k+1
X.interaction.list[[m]][[k]] = X.list[[m]][,i]*X.list[[m]][,j]
}
}
}
X.interaction.list[[m]] = do.call(cbind, X.interaction.list[[m]])
#  X.plus.inter.list[[m]] = cbind(X.list[[m]], X.interaction.list[[m]])
}
X1.interaction.mean = apply(X.interaction.list[[1]], 2, mean)
X1.interaction.sd = apply(X.interaction.list[[1]], 2, sd)
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.mean)
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.sd, "/")
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.mean)
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.sd, "/")
# # if the std is really small just subtract the mean in the following step
# X1.interaction.sd = sapply(X1.interaction.sd, function(x) ifelse(x<1e-5, 1, x))
# X.interaction.list[[1]] = t(apply(X.interaction.list[[1]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
# X.interaction.list[[2]] = t(apply(X.interaction.list[[2]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
X.plus.inter.list = lapply(1:2, function(x) cbind(X.list[[x]], X.interaction.list[[x]]))
# --- computing distance correlation to select favorite number of features including interaction features ---
# number of features to be selected
p_dc = 200
cl = makeCluster(4) # number of cores you can use
registerDoParallel(cl)
dc.vec = foreach(col_ix = 1:ncol(X.plus.inter.list[[1]]), .packages = "energy", .combine = "c") %dopar% {
dcor(Y.list[[1]], X.plus.inter.list[[1]][,col_ix])
}
stopCluster(cl)
# dc.vec = apply(X.plus.inter.list[[1]], 2, function(feature) dcor(Y.list[[1]], feature))
print("Finish calculation distance correlation")
X.selected.feature.id = order(dc.vec, decreasing = TRUE)[1:p_dc]
X.selected.feature.list = lapply(1:2, function(ix) X.plus.inter.list[[ix]][, X.selected.feature.id])
# add to the last... probably to fix a random forest bug
feature.ncol = ncol(X.selected.feature.list[[1]])
colnames(X.selected.feature.list[[1]]) = as.character(seq(1, feature.ncol))
colnames(X.selected.feature.list[[2]]) = as.character(seq(1, feature.ncol))
# ----------------------------------------- main -------------------------------------------
# ------------- parameters --------------
nfolds.log = 5 # ordinal logistic: Sl
nfolds.llr = 5 # local linear regression
alpha0 = 0
gamma.vec = exp(rev(seq(-2, 7, length.out = 50)))
# initial point for optimization, first K-1 parameters are thetas, then the first coming p are coefficients the last p are slack variable
initial.x = c(seq(-2, 2, length.out = length(levels(label.list[[1]]))-1), rep(0,p_dc), rep(1,p_dc))
measure.type = "corr"
# ------------------------ ordinal logistic: Sl --------------------------
ordinlog.list = cv.ordinlog.en(label.list[[1]], X.selected.feature.list[[1]], Y.list[[1]], gamma.vec, alpha0, initial.x, nfolds.log, "corr")
ordin.ml = ordinlog.list$ordin.ml
sl.list = lapply(1:2, function(x) as.vector(X.selected.feature.list[[x]]%*%ordin.ml$w))
# delete outliers
sl.list = lapply(1:2, function(ix){
sl.list[[ix]][sl.list[[ix]] > boxplot(sl.list[[ix]], plot = F)$stats[5,1]] = boxplot(sl.list[[ix]], plot = F)$stats[5,1]
sl.list[[ix]][sl.list[[ix]] < boxplot(sl.list[[ix]], plot = F)$stats[1,1]] = boxplot(sl.list[[ix]], plot = F)$stats[1,1]
sl.list[[ix]]
})
# tuning parameter Di for SlRf
Di.vec = seq(sd(sl.list[[1]])/5, sd(sl.list[[1]])*2, length.out = 20)
# ----------------------------- SlRf LWPR ---------------------------------
par.list = cv.bothPen.noDb.nolambda(label.list[[1]], X.selected.feature.list[[1]], Y.list[[1]], alpha, nfolds.llr, sl.list[[1]], Di.vec)
source('~/GitHub/LWPR/function/fun.rfguided.R', echo=TRUE)
source('~/GitHub/LWPR/function/ordinlog1.R', echo=TRUE)
source('~/GitHub/LWPR/function/fun.noDb.R', echo=TRUE)
# ----------------------------- SlRf LWPR ---------------------------------
par.list = cv.bothPen.noDb.nolambda(label.list[[1]], X.selected.feature.list[[1]], Y.list[[1]], alpha, nfolds.llr, sl.list[[1]], Di.vec)
par.list
Di.selected = par.list$Di
lam.id = par.list$lambda.id
id.which = par.list$id.which
if(id.which == 1){
ml.rf = randomForest(x = X.selected.feature.list[[1]], y = Y.list[[1]], keep.inbag = T, ntree = 100)
wrf.list = rf.weight(ml.rf, X.selected.feature.list[[1]], X.selected.feature.list[[2]])
mymethod.res = SlRf.weight.noDb(wrf.list, Y.list[[1]], sl.list[[1]], sl.list[[2]], Di.selected)
}else{
mymethod.res = slnp.noDb(X.selected.feature.list[[1]], Y.list[[1]], sl.list[[1]], X.selected.feature.list[[2]], Y.list[[2]], sl.list[[2]], Di.selected)
}
Yhat.mymethod = mymethod.res$Yhat
rwrf.list = mymethod.res$w.list
Yhat.mymethodPen = predict.penalized.origin.method.nolambda(X.selected.feature.list[[1]], Y.list[[1]], X.selected.feature.list[[2]], rwrf.list, lam.id, alpha)
mae.mymethod = mean(abs(Yhat.mymethod - exp(Y.list[[2]])))
corr.mymethod = cor(Yhat.mymethod, exp(Y.list[[2]]))
mae.mymethodPen = mean(abs(Yhat.mymethodPen - exp(Y.list[[2]])))
t
a
corr.mymethod = cor(Yhat.mymethod, exp(Y.list[[2]]))
corr.mymethodPen = cor(Yhat.mymethodPen, exp(Y.list[[2]]))
Y.list[[1]]
Y.list[[2]]
exp(Y.list[[2]])
Yhat.mymethod
# ------------------------------ Caculating results ----------------------------------------
# -------------------- mae and corr --------------------------
Yhat.mymethod = exp(Yhat.mymethod)
Yhat.mymethodPen = exp(Yhat.mymethodPen)
mae.mymethod = mean(abs(Yhat.mymethod - exp(Y.list[[2]])))
corr.mymethod = cor(Yhat.mymethod, exp(Y.list[[2]]))
mae.mymethodPen = mean(abs(Yhat.mymethodPen - exp(Y.list[[2]])))
corr.mymethodPen = cor(Yhat.mymethodPen, exp(Y.list[[2]]))
source('~/GitHub/LWPR/function/fun.rfguided.R', echo=TRUE)
Yhat.mymethodPen = predict.penalized.origin.method.nolambda(X.selected.feature.list[[1]], Y.list[[1]], X.selected.feature.list[[2]], rwrf.list, lam.id, alpha)
predict.penalized.origin.method.nolambda = function(X.train, Y.train, X.test, wrf.list, lam.id, alpha){
n.test = dim(X.test)[1]
p = dim(X.test)[2]
nlambda = 100
#diff.mtx.list = diff.matrix(X.train, X.test)
diff.mtx.list = diff.matrix.simple(X.train, X.test)
#diff.mtx.list = diff.matrix.weight.standardize(X.train, X.test, wrf.list)
beta = c()
for (x in 1:n.test){
if(sd(Y.train[wrf.list[[x]]>1e-5])==0 | length(Y.train[wrf.list[[x]]>1e-5])<=10){
print(1)
beta[x] = sum(Y.train*wrf.list[[x]])
}
else{
print(2)
lam_max = lambda.max(diff.mtx.list[[x]], Y.train, wrf.list[[x]], alpha)
lambda.vec = exp(seq(log(lam_max), log(lam_max*1e-3), length.out = nlambda))
fit = glmnet(x = diff.mtx.list[[x]], y = Y.train, weights = wrf.list[[x]], alpha = alpha, lambda = lambda.vec, standardize = F)
# print(lambda.vec[lam.id])
print(as.matrix(coef(fit, s=lambda.vec)))
beta.x = as.vector(coef(fit, s=lambda.vec[lam.id]))
beta[x] = beta.x[1]}
}
return(beta)
}
Yhat.mymethodPen = predict.penalized.origin.method.nolambda(X.selected.feature.list[[1]], Y.list[[1]], X.selected.feature.list[[2]], rwrf.list, lam.id, alpha)
predict.penalized.origin.method.nolambda = function(X.train, Y.train, X.test, wrf.list, lam.id, alpha){
n.test = dim(X.test)[1]
p = dim(X.test)[2]
nlambda = 100
#diff.mtx.list = diff.matrix(X.train, X.test)
diff.mtx.list = diff.matrix.simple(X.train, X.test)
#diff.mtx.list = diff.matrix.weight.standardize(X.train, X.test, wrf.list)
beta = c()
for (x in 1:n.test){
if(sd(Y.train[wrf.list[[x]]>1e-5])==0 | length(Y.train[wrf.list[[x]]>1e-5])<=10){
print(1)
beta[x] = sum(Y.train*wrf.list[[x]])
}
else{
print(2)
lam_max = lambda.max(diff.mtx.list[[x]], Y.train, wrf.list[[x]], alpha)
lambda.vec = exp(seq(log(lam_max), log(lam_max*1e-3), length.out = nlambda))
fit = glmnet(x = diff.mtx.list[[x]], y = Y.train, weights = wrf.list[[x]], alpha = alpha, lambda = lambda.vec, standardize = F)
# print(lambda.vec[lam.id])
print(as.matrix(coef(fit, s=lambda.vec))[1,])
beta.x = as.vector(coef(fit, s=lambda.vec[lam.id]))
beta[x] = beta.x[1]}
}
return(beta)
}
Yhat.mymethodPen = predict.penalized.origin.method.nolambda(X.selected.feature.list[[1]], Y.list[[1]], X.selected.feature.list[[2]], rwrf.list, lam.id, alpha)
alpha
