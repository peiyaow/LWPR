<<<<<<< HEAD
alpha0[n_sub, N_out] = -sum(alpha0_raw[n_sub,]);
}
for(n_sub in 1:N_sub){
for(n_out in 1:(N_out-1)){
alpha0[n_sub, n_out] = alpha0_raw[n_sub, n_out];
}
alpha0[n_sub, N_out] = -sum(alpha0_raw[n_sub,]);
}
for(n_sub in 1:N_sub){
for(n_out in 1:(N_out-1)){
alpha0[n_sub, n_out] = alpha0_raw[n_sub, n_out];
}
alpha0[n_sub, N_out] = -sum(alpha0_raw[n_sub,]);
}
n_sub
n_out
alpha1
alpha0_raw
alpha0
for(n_sub in 1:N_sub){
for(n_out in 1:(N_out-1)){
alpha0[n_sub, n_out] = alpha0_raw[n_sub, n_out];
}
alpha0[n_sub, N_out] = -sum(alpha0_raw[n_sub,]);
}
alpha0
for(n_sub in 1:N_sub){
for(n_out in 1:(N_out-1)){
alpha0[n_sub, n_out] = alpha0_raw[n_sub, n_out];
}
alpha0[n_sub, N_out] = -sum(alpha0_raw[n_sub,]);
}
alpha0_raw
alpha0_raw[n_sub, n_out]
alpha0[n_sub, n_out]
alpha0[n_sub, N_out]
for(n_sub in 1:N_sub){
for(n_out in 1:(N_out-1)){
alpha0[n_sub, n_out] = alpha0_raw[n_sub, n_out];
}
alpha0[n_sub, N_out] = -sum(alpha0_raw[n_sub,]);
}
alpha0 = matrix(0, ncol = N_out, nrow = N_sub)
for(n_sub in 1:N_sub){
for(n_out in 1:(N_out-1)){
alpha0[n_sub, n_out] = alpha0_raw[n_sub, n_out];
}
alpha0[n_sub, N_out] = -sum(alpha0_raw[n_sub,]);
}
alpha0
for(n_sub in 1:N_sub){
for(n_out in 1:(N_out-1)){
alpha0[n_sub, n_out] = alpha0_raw[n_sub, n_out];
}
alpha0[n_sub, N_out] = -sum(alpha0_raw[n_sub,]);
}
source('~/Desktop/mdonohue-ltjmm-5ef87d844807/R/ltjmm.R', echo=TRUE)
dd2 <- simulate.ltjmm(setup,
beta = beta,
gamma = gamma,
sigma_diag = sigma_alpha,
sigma_delta = sigma_delta,
sigma_y = sigma_y,
seed = 201610014)
dd2
dd$Y <- dd2$y
dd$Q <- dd$Z <- dd$Z2 <- NA
for(oc in unique(dd$outcome)){
subs <- dd$outcome == oc
ecdf.fun <- ecdf(dd$Y[subs])
dd$Q[subs] <- ecdf.fun(dd$Y[subs])
dd$Z2[subs] <- qnorm(dd$Q[subs])
dd$Z[subs] <- scale(dd$Y[subs])
}
fit <- stan(file = file.path(.libPaths()[1], "ltjmm", "stan", "ltjmm.stan"),
seed = rng_seed,
data = ltjmm(Y ~ year |
1 | # fixed effects direct on outcome
id | outcome,
data = dd)$data,
pars = c('beta', 'delta', 'alpha0', 'alpha1', 'gamma',
'sigma_alpha0', 'sigma_alpha1', "sigma_delta", 'sigma_y', 'log_lik'),
open_progress = FALSE, chains = 2, iter = 2000,
warmup = 1000, thin = 1, cores = 2)
.libPaths()[1]
fit
subjects
fit$`beta[1,1]`
fit$beta[1,1]
fit$beta
fit$`delta[1]`
fit$`alpha0[395,4]`
fit[["delta[1]"]]
fit[["delta"]]
fit
delta(fit)
fit$`beta[1,1]`
fit beta[1,1]`
fit
par_dims(fit)
fit@par_dims
getwd()
setwd("/Users/MonicaW/Documents/GitHub/LWPR/")
setwd("/Users/MonicaW/Documents/GitHub/LWPR/data")
X0 = as.matrix(read_table("X1a.txt", col_names = F))
Y0 = as.matrix(read_table("Y5T.txt", col_names = F))[, 4+5*(t-1)]
Y0
t
t = 1
Y0 = as.matrix(read_table("Y5T.txt", col_names = F))[, 4+5*(t-1)]
Y0
Y0 = as.matrix(read_table("Y5T.txt", col_names = F))[, 4+5*(c(1,2,3)-1)]
Y0
apply(Y0, 2, function(x) x[is.na(x)])
apply(Y0, 2, function(x) x[x<0])
apply(Y0, 2, function(x) x[x<0] = NA x)
apply(Y0, 2, function(x) {x[x<0] = NA
x})
Y0 = apply(Y0, 2, function(x) {x[x<0] = NA
x})
subjects
subject_time_outcome
expand.grid(id = 1:n, visit = c(1, 2, 3, 4), outcome = 1:p)
n = nrow(X0)
n
sample(n,200)
X = X0[sample(n,200),]
X
Y = Y0[id,]
label0 = as.ordered(read_table("label1.txt", col_names = F)$X1)
dd$apoe
Xs = X0[, 1:186]
Xs.med.list = lapply(levels(label0), function(x) apply(Xs[label0 == x, ], 2, function(y) median(y, na.rm = T)))
Xs.missing = is.na(Xs)
for (i in 1:nrow(Xs)){
ip.med = Xs.med.list[[as.numeric(label0[i])]]
Xs[i, Xs.missing[i, ]] = ip.med[Xs.missing[i, ]]
}
n = nrow(X0)
id = sample(n,200)
X = Xs[id,]
Y = Y0[id,]
X
Y
subject_time_outcome = expand.grid(id = 1:n, visit = c(1, 2, 3, 4), outcome = 1:p)
subject_time_outcome
subject_time_outcome = expand.grid(id = id, visit = c(1, 2, 3, 4), outcome = 1:p)
subject_time_outcome
Y
X
subject_time_outcome
is.na(Y[i,])
i  =1
is.na(Y[i,])
Y[i,][!is.na(Y[i,])]
is.matrix(Y)
mylist = list()
for (i in 1:n){
outcome = Y[i,][!is.na(Y[i,])]
year = c(1,2,3)[!is.na(Y[i,])]
mylist[[i]] = cbind(year, outcome)
}
Y
n
n = nrow(X)
mylist = list()
for (i in 1:n){
outcome = Y[i,][!is.na(Y[i,])]
year = c(1,2,3)[!is.na(Y[i,])]
mylist[[i]] = cbind(year, outcome)
}
mylist
mylist = list()
for (i in 1:n){
id = id[i]
outcome = Y[i,][!is.na(Y[i,])]
year = c(1,2,3)[!is.na(Y[i,])]
mylist[[i]] = cbind(id,year, outcome)
}
mylist
id
id = sample(n,200)
X = Xs[id,]
Y = Y0[id,]
n = nrow(X)
mylist = list()
for (i in 1:n){
id = id[i]
outcome = Y[i,][!is.na(Y[i,])]
year = c(1,2,3)[!is.na(Y[i,])]
mylist[[i]] = cbind(id,year, outcome)
}
mylist
id
id = sample(805,200)
id
X = Xs[id,]
Y = Y0[id,]
n = nrow(X)
mylist = list()
for (i in 1:n){
id = id[i]
outcome = Y[i,][!is.na(Y[i,])]
year = c(1,2,3)[!is.na(Y[i,])]
mylist[[i]] = cbind(id,year, outcome)
}
mylist
id[i]
id
id = sample(805,200)
X = Xs[id,]
Y = Y0[id,]
n = nrow(X)
mylist = list()
mylist = list()
for (i in 1:n){
outcome = Y[i,][!is.na(Y[i,])]
year = c(1,2,3)[!is.na(Y[i,])]
mylist[[i]] = cbind(id = id[i],year, outcome)
}
mylist
do.call(rbind, mylist)
subjects
library(dplyr)
id = sample(805,200)
X = Xs[id,]
Y = Y0[id,]
n = nrow(X)
Ylist = list()
for (i in 1:n){
outcome = Y[i,][!is.na(Y[i,])]
year = c(1,2,3)[!is.na(Y[i,])]
Ylist[[i]] = cbind(id = id[i],year, outcome)
}
do.call(rbind, Ylist)
id_year_outcome = do.call(rbind, Ylist)
id_year_outcome
id_X = cbind(id, X)
id_X
right_join(id_X, id_year_outcome, by = 'id')
id_X
id_X = as.data.frame(cbind(id, X))
id_X
id_X
id_X
id_year_outcome = as.data.frame(do.call(rbind, Ylist))
right_join(id_X, id_year_outcome, by = 'id')
subjects
dd
rng_seed <- 20161001
set.seed(rng_seed)
n <- 400 # subjects
p <- 4 # outcomes
t <- 4 # time points
subjects <- data.frame(id = 1:n,
age.0 = rnorm(n, 75, 5),
apoe = sample(c(0, 1), size = n, replace = TRUE))
subject_time_outcome <- expand.grid(id = 1:n, visit = c(1, 2, 3, 4), outcome = 1:p)
subject_time_outcome <- subject_time_outcome[order(subject_time_outcome$id,
subject_time_outcome$outcome),]
for(i in 1:n){
time.year <- sort(runif(t, 0, 10)) # t number of time points
for(j in 1:p){
subject_time_outcome$year[(subject_time_outcome$id == i) &
(subject_time_outcome$outcome == j)] <- time.year
}
}
dd <- right_join(subjects, subject_time_outcome, by = 'id')
dd
dd$Y <- NA
dd0 <- dd
setup <- ltjmm(Y ~ year | 1 | id | outcome, data = dd)
setup
Y.list = list()
for (i in 1:n){
Y.tmp = Y[i,][!is.na(Y[i,])]
year = c(1,2,3)[!is.na(Y[i,])]
Y.list[[i]] = cbind(id = id[i], year, Y = Y.tmp)
}
id = sample(805,200)
X = Xs[id,]
Y = Y0[id,]
n = nrow(X)
Y.list = list()
id_year_Y.list = list()
for (i in 1:n){
Y.tmp = Y[i,][!is.na(Y[i,])]
year = c(1,2,3)[!is.na(Y[i,])]
id_year_Y.list[[i]] = cbind(id = id[i], year, Y = Y.tmp)
}
id_year_Y = as.data.frame(do.call(rbind, id_year_Y.list))
id_year_Y
id_X = as.data.frame(cbind(id, X))
id_X
id_year_Y_outcome = data.frame(id_year_Y, outcome = 1)
id_year_Y_outcome
id_X = as.data.frame(cbind(id, X))
right_join(id_X, id_year_outcome, by = 'id')
right_join(id_X, id_year_Y_outcome, by = 'id')
dd = right_join(id_X, id_year_Y_outcome, by = 'id')
dd
right_join(subjects, subject_time_outcome, by = 'id')
id = sample(805,200)
X = Xs[id,]
Y = Y0[id,]
n = nrow(X)
id_year_Y.list = list()
for (i in 1:n){
Y.tmp = Y[i,][!is.na(Y[i,])]
year = c(0,1,2)[!is.na(Y[i,])]
id_year_Y.list[[i]] = cbind(id = id[i], year, Y = Y.tmp)
}
id_year_Y = as.data.frame(do.call(rbind, id_year_Y.list))
id_year_Y_outcome = data.frame(id_year_Y, outcome = 1)
id
X
Y
n
id_year_Y.list = list()
for (i in 1:n){
Y.tmp = Y[i,][!is.na(Y[i,])]
year = c(0,1,2)[!is.na(Y[i,])]
id_year_Y.list[[i]] = cbind(id = id[i], year, Y = Y.tmp)
}
do.call(rbind, id_year_Y.list)
id_year_Y.list
id_year_Y = as.data.frame(do.call(rbind, id_year_Y.list))
id_year_Y.list
id_year_Y.list[[1]]
sapply(id_year_Y.list, ncol)
seq(1,200)[sapply(id_year_Y.list, ncol)==1]
id_year_Y.list[[93]]
seq(1,200)[id == 31]
i = 93
Y[i,][!is.na(Y[i,])]
!is.na(Y[i,]
)
Y0
apply(Y0, 1, is.na)
Y0 = apply(Y0, 1, !is.na)
apply(Y0, 1, is.na)
is.na(Y0[1,])
Y0
is.na(Y0[1,])
product(is.na(Y0[1,]))
prod(is.na(Y0[1,]))
prod(!is.na(Y0[1,]))
apply(Y0, 1, function(row) prod(!is.na(row)))
Y0
apply(Y0, 1, function(row) prod(!is.na(row)))
apply(Y0, 1, function(row) prod(is.na(row)))
apply(Y0, 1, function(row) !prod(is.na(row)))
apply(Y0, 1, function(row) !prod(is.na(row)))
Y0 = Y0[apply(Y0, 1, function(row) !prod(is.na(row))), ]
Xs = X0[, 1:186]
Xs.med.list = lapply(levels(label0), function(x) apply(Xs[label0 == x, ], 2, function(y) median(y, na.rm = T)))
Xs.missing = is.na(Xs)
allnan = apply(Y0, 1, function(row) !prod(is.na(row)))
Y0 = apply(Y0, 2, function(x) {x[x<0] = NA
x})
setwd("/Users/MonicaW/Documents/GitHub/LWPR/data")
X0 = as.matrix(read_table("X1a.txt", col_names = F))
Y0 = as.matrix(read_table("Y5T.txt", col_names = F))[, 4+5*(c(1,2,3)-1)]
label0 = as.ordered(read_table("label1.txt", col_names = F)$X1)
setwd("/Users/MonicaW/Documents/GitHub/LWPR/data")
X0 = as.matrix(read_table("X1a.txt", col_names = F))
Y0 = as.matrix(read_table("Y5T.txt", col_names = F))[, 4+5*(c(1,2,3)-1)]
label0 = as.ordered(read_table("label1.txt", col_names = F)$X1)
Y0 = apply(Y0, 2, function(x) {x[x<0] = NA
x})
allnan = apply(Y0, 1, function(row) !prod(is.na(row)))
Y0 = Y0[allnan, ]
Xs = X0[allnan, 1:186]
Xs.med.list = lapply(levels(label0), function(x) apply(Xs[label0 == x, ], 2, function(y) median(y, na.rm = T)))
label0 = label0[allnan]
Xs.med.list = lapply(levels(label0), function(x) apply(Xs[label0 == x, ], 2, function(y) median(y, na.rm = T)))
=======
X.list[[2]] = sweep(X.list[[2]], 2, X1.mean)
X.list[[2]] = sweep(X.list[[2]], 2, X1.sd, "/")
apply(X.list[[2]], 2, sd)
Y.list = lapply(1:2, function(x) Y[id[[x]]]) # train,test
X.list = lapply(1:2, function(x) X[id[[x]],])
label.list = lapply(1:2, function(x) label[id[[x]]])
# ----------------------------------- do scale ----------------------------------
X1.mean = apply(X.list[[1]], 2, mean)
X1.sd = apply(X.list[[1]], 2, sd)
X.list[[1]] = sweep(X.list[[1]], 2, X1.mean)
X.list[[1]] = sweep(X.list[[1]], 2, X1.sd, "/")
X.list[[2]] = sweep(X.list[[2]], 2, X1.mean)
X.list[[2]] = sweep(X.list[[2]], 2, X1.sd, "/")
X.list
# ----------------------------- Calculating interaction term ------------------------------
X.interaction.list = list()
X.plus.inter.list = list()
for (m in 1:2){
X.interaction.list[[m]] = list()
k = 0
for (i in 1:p){
for (j in 1:p){
if (i < j){
k = k+1
X.interaction.list[[m]][[k]] = X.list[[m]][,i]*X.list[[m]][,j]
}
}
}
X.interaction.list[[m]] = do.call(cbind, X.interaction.list[[m]])
#  X.plus.inter.list[[m]] = cbind(X.list[[m]], X.interaction.list[[m]])
}
X1.interaction.mean = apply(X.interaction.list[[1]], 2, mean)
X1.interaction.sd = apply(X.interaction.list[[1]], 2, sd)
X1.interaction.sd
X1.interaction.mean
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.mean)
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.sd, "/")
X.interaction.list
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.mean)
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.sd, "/")
apply(X.interaction.list[[1]], 2, mean)
apply(X.interaction.list[[1]], 2, sd)
apply(X.interaction.list[[2]], 2, sd)
# ----------------------------- Calculating interaction term ------------------------------
X.interaction.list = list()
X.plus.inter.list = list()
for (m in 1:2){
X.interaction.list[[m]] = list()
k = 0
for (i in 1:p){
for (j in 1:p){
if (i < j){
k = k+1
X.interaction.list[[m]][[k]] = X.list[[m]][,i]*X.list[[m]][,j]
}
}
}
X.interaction.list[[m]] = do.call(cbind, X.interaction.list[[m]])
#  X.plus.inter.list[[m]] = cbind(X.list[[m]], X.interaction.list[[m]])
}
X1.interaction.mean = apply(X.interaction.list[[1]], 2, mean)
X1.interaction.sd = apply(X.interaction.list[[1]], 2, sd)
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.mean)
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.sd, "/")
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.mean)
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.sd, "/")
apply(X.interaction.list[[2]], 2, sd)
apply(X.interaction.list[[2]], 2, mean)
# if the std is really small just subtract the mean in the following step
# X1.interaction.sd = sapply(X1.interaction.sd, function(x) ifelse(x<1e-5, 1, x))
# X.interaction.list[[1]] = t(apply(X.interaction.list[[1]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
# X.interaction.list[[2]] = t(apply(X.interaction.list[[2]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
X.plus.inter.list = lapply(1:2, function(x) cbind(X.list[[x]], X.interaction.list[[x]]))
# --- computing distance correlation to select favorite number of features including interaction features ---
# number of features to be selected
p_dc = 200
cl = makeCluster(4) # number of cores you can use
registerDoParallel(cl)
dc.vec = foreach(col_ix = 1:ncol(X.plus.inter.list[[1]]), .packages = "energy", .combine = "c") %dopar% {
dcor(Y.list[[1]], X.plus.inter.list[[1]][,col_ix])
}
stopCluster(cl)
X.selected.feature.id = order(dc.vec, decreasing = TRUE)[1:p_dc]
X.selected.feature.id
X.selected.feature.list = lapply(1:2, function(ix) X.plus.inter.list[[ix]][, X.selected.feature.id])
# add to the last... probably to fix a random forest bug
feature.ncol = ncol(X.selected.feature.list[[1]])
colnames(X.selected.feature.list[[1]]) = as.character(seq(1, feature.ncol))
colnames(X.selected.feature.list[[2]]) = as.character(seq(1, feature.ncol))
feature.ncol
# ----------------------------------------- main -------------------------------------------
# ------------- parameters --------------
nfolds = 5
alpha0 = 0
gamma.vec = exp(rev(seq(-2, 7, length.out = 50)))
# initial point for optimization, first K-1 parameters are thetas, then the first coming p are coefficients the last p are slack variable
initial.x = c(seq(-2, 2, length.out = length(levels(label.list[[1]]))-1), rep(0,p_dc), rep(1,p_dc))
lDi = 20
ordinlog.list = cv.ordinlog.both.noDb(label.list[[1]], X.selected.feature.list[[1]], Y.list[[1]], gamma.vec, alpha0, initial.x, nfolds, lDi)
ordin.ml = ordinlog.list$ordin.ml
sl.list = lapply(1:2, function(x) as.vector(X.selected.feature.list[[x]]%*%ordin.ml$w))
# delete outliers
sl.list = lapply(1:2, function(ix){
sl.list[[ix]][sl.list[[ix]] > boxplot(sl.list[[ix]], plot = F)$stats[5,1]] = boxplot(sl.list[[ix]], plot = F)$stats[5,1]
sl.list[[ix]][sl.list[[ix]] < boxplot(sl.list[[ix]], plot = F)$stats[1,1]] = boxplot(sl.list[[ix]], plot = F)$stats[1,1]
sl.list[[ix]]
})
Di.vec = seq(sd(sl.list[[1]])/5, sd(sl.list[[1]])*2, length.out = lDi)
Di.selected = Di.vec[ordinlog.list$Di.id]
id.which = ordinlog.list$id
if(id.which == 1){
ml.rf = randomForest(x = X.selected.feature.list[[1]], y = Y.list[[1]], keep.inbag = T, ntree = 100)
wrf.list = rf.weight(ml.rf, X.selected.feature.list[[1]], X.selected.feature.list[[2]])
mymethod.res = SlRf.weight.noDb(wrf.list, Y.list[[1]], sl.list[[1]], sl.list[[2]], Di.selected)
}else{
mymethod.res = slnp.noDb(X.selected.feature.list[[1]], Y.list[[1]], sl.list[[1]], X.selected.feature.list[[2]], Y.list[[2]], sl.list[[2]], Di.selected)
}
print("Finish local fitting without penalization")
Yhat.mymethod = mymethod.res$Yhat
Yhat.mymethod = exp(Yhat.mymethod)
mae.mymethod = mean(abs(Yhat.mymethod - exp(Y.list[[2]])))
gamma.vec
lDi
corr.mymethod = cor(Yhat.mymethod, exp(Y.list[[2]]))
Y.list = lapply(1:2, function(x) Y[id[[x]]]) # train,test
X.list = lapply(1:2, function(x) X[id[[x]],])
label.list = lapply(1:2, function(x) label[id[[x]]])
# ----------------------------------- do scale ----------------------------------
X1.mean = apply(X.list[[1]], 2, mean)
X1.sd = apply(X.list[[1]], 2, sd)
X.list[[1]] = sweep(X.list[[1]], 2, X1.mean)
X.list[[1]] = sweep(X.list[[1]], 2, X1.sd, "/")
X.list[[2]] = sweep(X.list[[2]], 2, X1.mean)
X.list[[2]] = sweep(X.list[[2]], 2, X1.sd, "/")
# ----------------------------- Calculating interaction term ------------------------------
X.interaction.list = list()
X.plus.inter.list = list()
for (m in 1:2){
X.interaction.list[[m]] = list()
k = 0
for (i in 1:p){
for (j in 1:p){
if (i < j){
k = k+1
X.interaction.list[[m]][[k]] = X.list[[m]][,i]*X.list[[m]][,j]
}
}
}
X.interaction.list[[m]] = do.call(cbind, X.interaction.list[[m]])
#  X.plus.inter.list[[m]] = cbind(X.list[[m]], X.interaction.list[[m]])
}
X1.interaction.mean = apply(X.interaction.list[[1]], 2, mean)
X1.interaction.sd = apply(X.interaction.list[[1]], 2, sd)
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.mean)
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.sd, "/")
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.mean)
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.sd, "/")
# # if the std is really small just subtract the mean in the following step
# X1.interaction.sd = sapply(X1.interaction.sd, function(x) ifelse(x<1e-5, 1, x))
# X.interaction.list[[1]] = t(apply(X.interaction.list[[1]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
# X.interaction.list[[2]] = t(apply(X.interaction.list[[2]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
X.plus.inter.list = lapply(1:2, function(x) cbind(X.list[[x]], X.interaction.list[[x]]))
# --- computing distance correlation to select favorite number of features including interaction features ---
# number of features to be selected
p_dc = 200
cl = makeCluster(4) # number of cores you can use
registerDoParallel(cl)
dc.vec = foreach(col_ix = 1:ncol(X.plus.inter.list[[1]]), .packages = "energy", .combine = "c") %dopar% {
dcor(Y.list[[1]], X.plus.inter.list[[1]][,col_ix])
}
stopCluster(cl)
# dc.vec = apply(X.plus.inter.list[[1]], 2, function(feature) dcor(Y.list[[1]], feature))
print("Finish calculation distance correlation")
X.selected.feature.id = order(dc.vec, decreasing = TRUE)[1:p_dc]
X.selected.feature.list = lapply(1:2, function(ix) X.plus.inter.list[[ix]][, X.selected.feature.id])
write.table(X.selected.feature.id, paste0("feature_id+", name.id, ".txt"), sep="\t", row.names = F, col.names = F, append = T)
print("Finish addig top interaction terms into design matrix")
# add to the last... probably to fix a random forest bug
feature.ncol = ncol(X.selected.feature.list[[1]])
colnames(X.selected.feature.list[[1]]) = as.character(seq(1, feature.ncol))
colnames(X.selected.feature.list[[2]]) = as.character(seq(1, feature.ncol))
# ----------------------------------------- main -------------------------------------------
# ------------- parameters --------------
nfolds.log = 5 # ordinal logistic: Sl
nfolds.llr = 5 # local linear regression
alpha0 = 0
gamma.vec = exp(rev(seq(-2, 7, length.out = 50)))
# initial point for optimization, first K-1 parameters are thetas, then the first coming p are coefficients the last p are slack variable
initial.x = c(seq(-2, 2, length.out = length(levels(label.list[[1]]))-1), rep(0,p_dc), rep(1,p_dc))
measure.type = "corr"
# ------------------------ ordinal logistic: Sl --------------------------
ordinlog.list = cv.ordinlog.en(label.list[[1]], X.selected.feature.list[[1]], Y.list[[1]], gamma.vec, alpha0, initial.x, nfolds.log, "corr")
ordin.ml = ordinlog.list$ordin.ml
sl.list = lapply(1:2, function(x) as.vector(X.selected.feature.list[[x]]%*%ordin.ml$w))
# delete outliers
sl.list = lapply(1:2, function(ix){
sl.list[[ix]][sl.list[[ix]] > boxplot(sl.list[[ix]], plot = F)$stats[5,1]] = boxplot(sl.list[[ix]], plot = F)$stats[5,1]
sl.list[[ix]][sl.list[[ix]] < boxplot(sl.list[[ix]], plot = F)$stats[1,1]] = boxplot(sl.list[[ix]], plot = F)$stats[1,1]
sl.list[[ix]]
})
# tuning parameter Di for SlRf
Di.vec = seq(sd(sl.list[[1]])/5, sd(sl.list[[1]])*2, length.out = 20)
# ----------------------------- SlRf LWPR ---------------------------------
par.list = cv.bothPen.noDb.nolambda(label.list[[1]], X.selected.feature.list[[1]], Y.list[[1]], alpha, nfolds.llr, sl.list[[1]], Di.vec)
par.list
Di.selected = par.list$Di
lam.id = par.list$lambda.id
id.which = par.list$id.which
if(id.which == 1){
ml.rf = randomForest(x = X.selected.feature.list[[1]], y = Y.list[[1]], keep.inbag = T, ntree = 100)
wrf.list = rf.weight(ml.rf, X.selected.feature.list[[1]], X.selected.feature.list[[2]])
mymethod.res = SlRf.weight.noDb(wrf.list, Y.list[[1]], sl.list[[1]], sl.list[[2]], Di.selected)
}else{
mymethod.res = slnp.noDb(X.selected.feature.list[[1]], Y.list[[1]], sl.list[[1]], X.selected.feature.list[[2]], Y.list[[2]], sl.list[[2]], Di.selected)
}
print("Finish local fitting without penalization")
Yhat.mymethod = mymethod.res$Yhat
rwrf.list = mymethod.res$w.list
Yhat.mymethodPen = predict.penalized.origin.method.nolambda(X.selected.feature.list[[1]], Y.list[[1]], X.selected.feature.list[[2]], rwrf.list, lam.id, alpha)
# ------------------------------ Caculating results ----------------------------------------
# -------------------- mae and corr --------------------------
Yhat.mymethod = exp(Yhat.mymethod)
Yhat.mymethodPen = exp(Yhat.mymethodPen)
mae.mymethod = mean(abs(Yhat.mymethod - exp(Y.list[[2]])))
corr.mymethod = cor(Yhat.mymethod, exp(Y.list[[2]]))
mae.mymethodPen = mean(abs(Yhat.mymethodPen - exp(Y.list[[2]])))
corr.mymethodPen = cor(Yhat.mymethodPen, exp(Y.list[[2]]))
mae.mymethod
mae.mymethodPen
predict.penalized.origin.method.nolambda = function(X.train, Y.train, X.test, wrf.list, lam.id, alpha){
n.test = dim(X.test)[1]
p = dim(X.test)[2]
nlambda = 100
#diff.mtx.list = diff.matrix(X.train, X.test)
diff.mtx.list = diff.matrix.weight.standardize(X.train, X.test, wrf.list)
beta = c()
for (x in 1:n.test){
if(sd(Y.train[wrf.list[[x]]>1e-5])==0 | length(Y.train[wrf.list[[x]]>1e-5])<=10){
beta[x] = sum(Y.train*wrf.list[[x]])
}
else{
lam_max = lambda.max(diff.mtx.list[[x]], Y.train, wrf.list[[x]], alpha)
lambda.vec = exp(seq(log(lam_max), log(lam_max*1e-3), length.out = nlambda))
fit = glmnet(x = diff.mtx.list[[x]], y = Y.train, weights = wrf.list[[x]], alpha = alpha, lambda = lambda.vec, standardize = F)
# print(lambda.vec[lam.id])
beta.x = as.vector(coef(fit, s=lambda.vec[lam.id]))
print(beta.x)
beta[x] = beta.x[1]}
}
return(beta)
}
Yhat.mymethodPen = predict.penalized.origin.method.nolambda(X.selected.feature.list[[1]], Y.list[[1]], X.selected.feature.list[[2]], rwrf.list, lam.id, alpha)
source('~/GitHub/LWPR/function/fun.noDb.R', echo=TRUE)
source('~/GitHub/LWPR/function/ordinlog1.R', echo=TRUE)
source('~/GitHub/LWPR/function/fun.rfguided.R', echo=TRUE)
# ----------------------------------------- main -------------------------------------------
# ------------- parameters --------------
nfolds = 5
alpha0 = 0
gamma.vec = exp(rev(seq(-2, 7, length.out = 50)))
# initial point for optimization, first K-1 parameters are thetas, then the first coming p are coefficients the last p are slack variable
initial.x = c(seq(-2, 2, length.out = length(levels(label.list[[1]]))-1), rep(0,p_dc), rep(1,p_dc))
lDi = 20
ordinlog.list = cv.ordinlog.both.noDb(label.list[[1]], X.selected.feature.list[[1]], Y.list[[1]], gamma.vec, alpha0, initial.x, nfolds, lDi)
ordin.ml = ordinlog.list$ordin.ml
sl.list = lapply(1:2, function(x) as.vector(X.selected.feature.list[[x]]%*%ordin.ml$w))
# delete outliers
sl.list = lapply(1:2, function(ix){
sl.list[[ix]][sl.list[[ix]] > boxplot(sl.list[[ix]], plot = F)$stats[5,1]] = boxplot(sl.list[[ix]], plot = F)$stats[5,1]
sl.list[[ix]][sl.list[[ix]] < boxplot(sl.list[[ix]], plot = F)$stats[1,1]] = boxplot(sl.list[[ix]], plot = F)$stats[1,1]
sl.list[[ix]]
})
Di.vec = seq(sd(sl.list[[1]])/5, sd(sl.list[[1]])*2, length.out = lDi)
Di.selected = Di.vec[ordinlog.list$Di.id]
id.which = ordinlog.list$id
if(id.which == 1){
ml.rf = randomForest(x = X.selected.feature.list[[1]], y = Y.list[[1]], keep.inbag = T, ntree = 100)
wrf.list = rf.weight(ml.rf, X.selected.feature.list[[1]], X.selected.feature.list[[2]])
mymethod.res = SlRf.weight.noDb(wrf.list, Y.list[[1]], sl.list[[1]], sl.list[[2]], Di.selected)
}else{
mymethod.res = slnp.noDb(X.selected.feature.list[[1]], Y.list[[1]], sl.list[[1]], X.selected.feature.list[[2]], Y.list[[2]], sl.list[[2]], Di.selected)
}
print("Finish local fitting without penalization")
Yhat.mymethod = mymethod.res$Yhat
Yhat.mymethod = exp(Yhat.mymethod)
mae.mymethod = mean(abs(Yhat.mymethod - exp(Y.list[[2]])))
a = 0.5
t = 1
library(readr)
setwd("C:/Users/peiyao/Documents/GitHub/LWPR/data")
X0 = as.matrix(read_table("X1a.txt", col_names = F))
Y0 = as.matrix(read_table("Y5T.txt", col_names = F))[, 4+5*(t-1)]
label0 = as.ordered(read_table("label1.txt", col_names = F)$X1)
# remove NAs in Y
id.cut1 = !is.na(Y0)
X0 = X0[id.cut1, ]
Y0 = Y0[id.cut1]
label0 = label0[id.cut1]
# remove negatives in Y
id.cut2 = (Y0 > 0)
X0 = X0[id.cut2,]
Y0 = Y0[id.cut2]
label0 = label0[id.cut2]
# only select MRI+PET
Xs = X0[, 1:186]
# within group median , each element from the list is the median within the group
# list of length 3, each element 1 by 186 group median of each feature
Xs.med.list = lapply(levels(label0), function(x) apply(Xs[label0 == x, ], 2, function(y) median(y, na.rm = T)))
# impute with median
>>>>>>> 577b3e66b09e8ca9881c6553527d0abbffb58370
Xs.missing = is.na(Xs)
for (i in 1:nrow(Xs)){
ip.med = Xs.med.list[[as.numeric(label0[i])]]
Xs[i, Xs.missing[i, ]] = ip.med[Xs.missing[i, ]]
}
<<<<<<< HEAD
id = sample(805,200)
X = Xs[id,]
Y = Y0[id,]
n = nrow(X)
id_year_Y.list = list()
for (i in 1:n){
Y.tmp = Y[i,][!is.na(Y[i,])]
year = c(0,1,2)[!is.na(Y[i,])]
id_year_Y.list[[i]] = cbind(id = id[i], year, Y = Y.tmp)
}
id_year_Y = as.data.frame(do.call(rbind, id_year_Y.list))
id_year_Y_outcome = data.frame(id_year_Y, outcome = 1)
id_X = as.data.frame(cbind(id, X))
dd = right_join(id_X, id_year_Y_outcome, by = 'id')
dd
setup <- ltjmm(Y ~ year | .-Y-outcome-year-id | id | outcome, data = dd)
setup
n
fit <- stan(file = file.path(.libPaths()[1], "ltjmm", "stan", "ltjmm.stan"),
seed = rng_seed,
data = setup$data,
pars = c('beta', 'delta', 'alpha0', 'alpha1', 'gamma',
'sigma_alpha0', 'sigma_alpha1', "sigma_delta", 'sigma_y', 'log_lik'),
open_progress = FALSE, chains = 2, iter = 2000,
warmup = 1000, thin = 1, cores = 2)
fit <- stan(file = file.path(.libPaths()[1], "ltjmm", "stan", "ltjmm.stan"),
seed = rng_seed,
data = setup$data,
pars = c('beta', 'delta', 'alpha0', 'alpha1', 'gamma',
'sigma_alpha0', 'sigma_alpha1', "sigma_delta", 'sigma_y', 'log_lik'),
open_progress = FALSE, chains = 2, iter = 2000,
warmup = 1000, thin = 1, cores = 2)
fit
save.image("~/Documents/GitHub/LWPR/data/ltjmmfit.RData")
load("/Users/MonicaW/Documents/GitHub/LWPR/simulation/parameter/diag_cov/sim_par21.RData")
View(Sigma_1)
View(Sigma_c)
View(Sigma_c)
View(Sigma_0)
load("/Users/MonicaW/Documents/GitHub/LWPR/simulation/parameter/non_diag_cov/sim_par31.RData")
w
setup
library(dplyr)
library(readr)
library(ltjmm)
library(caret)
library(doParallel)
library(energy)
setwd("/Users/MonicaW/Documents/GitHub/LWPR/data")
X0 = as.matrix(read_table("X1a.txt", col_names = F))
Y0 = as.matrix(read_table("Y5T.txt", col_names = F))[, 4+5*(c(1,2,3)-1)]
label0 = as.ordered(read_table("label1.txt", col_names = F)$X1)
Y0 = apply(Y0, 2, function(x) {x[x<0] = NA
x})
allnan = apply(Y0, 1, function(row) !prod(is.na(row)))
Y0 = Y0[allnan, ]
Y0 = log(Y0+1)
Xs = X0[allnan, 1:186]
label0 = label0[allnan]
Xs.med.list = lapply(levels(label0), function(x) apply(Xs[label0 == x, ], 2, function(y) median(y, na.rm = T)))
Xs.missing = is.na(Xs)
for (i in 1:nrow(Xs)){
ip.med = Xs.med.list[[as.numeric(label0[i])]]
Xs[i, Xs.missing[i, ]] = ip.med[Xs.missing[i, ]]
}
n = nrow(Xs)
p = ncol(Xs)
set.seed(myseed)
idtrain = unlist(createDataPartition(label0, times = 1, p = 3/4))
idtest = (1:n)[-idtrain]
id_all = seq(1,n)
X1.mean = apply(Xs[idtrain,], 2, mean)
X1.sd = apply(Xs[idtrain,], 2, sd)
X1.sd = sapply(X1.sd, function(x) ifelse(x<1e-5, 1, x))
Xs[idtrain,] = t(apply(Xs[idtrain,], 1, function(x) (x - X1.mean)/X1.sd))
Xs[idtest,] = t(apply(Xs[idtest,], 1, function(x) (x - X1.mean)/X1.sd))
X.interaction.list = list()
=======
X = Xs
Y = Y0
label = label0
library(caret)
library(methods) # "is function issue by Rscript"
library(energy)
library(glmnet)
library(randomForest)
library(foreach)
library(doParallel)
source('~/GitHub/LWPR/function/fun.rfguided.R', echo=TRUE)
source('~/GitHub/LWPR/function/ordinlog1.R', echo=TRUE)
source('~/GitHub/LWPR/function/fun.noDb.R', echo=TRUE)
name.id = paste0("t=", as.character(t), "+alpha=", as.character(alpha))
# ---------------------- creating training and testing set -----------------------------
n = dim(X)[1]
p = dim(X)[2]
myseed = 551
idtrain = unlist(createDataPartition(label, times = 1, p = 3/4))
idtest = (1:n)[-idtrain]
Y = log(Y+1)
id = list(idtrain, idtest)
Y.list = lapply(1:2, function(x) Y[id[[x]]]) # train,test
X.list = lapply(1:2, function(x) X[id[[x]],])
label.list = lapply(1:2, function(x) label[id[[x]]])
# ----------------------------------- do scale ----------------------------------
X1.mean = apply(X.list[[1]], 2, mean)
X1.sd = apply(X.list[[1]], 2, sd)
X.list[[1]] = sweep(X.list[[1]], 2, X1.mean)
X.list[[1]] = sweep(X.list[[1]], 2, X1.sd, "/")
X.list[[2]] = sweep(X.list[[2]], 2, X1.mean)
X.list[[2]] = sweep(X.list[[2]], 2, X1.sd, "/")
# # if the std is really small just subtract the mean in the following step
# X1.sd = sapply(X1.sd, function(x) ifelse(x<1e-5, 1, x))
# X.list[[1]] = t(apply(X.list[[1]], 1, function(x) (x - X1.mean)/X1.sd))
# X.list[[2]] = t(apply(X.list[[2]], 1, function(x) (x - X1.mean)/X1.sd))
print("Finish scaling features")
# ----------------------------- Calculating interaction term ------------------------------
X.interaction.list = list()
X.plus.inter.list = list()
for (m in 1:2){
X.interaction.list[[m]] = list()
>>>>>>> 577b3e66b09e8ca9881c6553527d0abbffb58370
k = 0
for (i in 1:p){
for (j in 1:p){
if (i < j){
k = k+1
<<<<<<< HEAD
X.interaction.list[[k]] = Xs[,i]*Xs[,j]
}
}
}
X.interaction = do.call(cbind, X.interaction.list)
X1.interaction.mean = apply(X.interaction[idtrain,], 2, mean)
X1.interaction.sd = apply(X.interaction[idtrain,], 2, sd)
X1.interaction.sd = sapply(X1.interaction.sd, function(x) ifelse(x<1e-5, 1, x))
X.interaction[idtrain, ]= t(apply(X.interaction[idtrain, ], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
X.interaction[idtest, ] = t(apply(X.interaction[idtest, ], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
X.plus.inter = cbind(Xs, X.interaction)
p_dc = 200
cl = makeCluster(4) # number of cores you can use
registerDoParallel(cl)
dc.vec = foreach(col_ix = 1:ncol(X.plus.inter[idtrain, ]), .packages = "energy", .combine = "c") %dopar% {
dcor(Y0[idtrain], X.plus.inter[idtrain,][,col_ix])
}
stopCluster(cl)
X.selected.feature.id = order(dc.vec, decreasing = TRUE)[1:p_dc]
X.selected.feature = X.plus.inter[, X.selected.feature.id]
feature.ncol = ncol(X.selected.feature)
colnames(X.selected.feature) = as.character(seq(1, feature.ncol))
id_year_Y.list = list()
for (i in 1:n){
Y.tmp = Y0[i,][!is.na(Y0[i,])]
year = c(0,1,2)[!is.na(Y0[i,])]
id_year_Y.list[[i]] = cbind(id = id_all[i], year, Y = Y.tmp)
}
id_year_Y = as.data.frame(do.call(rbind, id_year_Y.list))
id_year_Y_outcome = data.frame(id_year_Y, outcome = 1)
id_X = as.data.frame(cbind(id = id_all, X.selected.feature))
dd = right_join(id_X, id_year_Y_outcome, by = 'id')
setup <- ltjmm(Y ~ year | .-Y-outcome-year-id | id | outcome, data = dd[dd$id %in% idtrain,])
fit <- stan(file = file.path(.libPaths()[1], "ltjmm", "stan", "ltjmm.stan"),
seed = rng_seed,
data = setup$data,
pars = c('beta', 'delta', 'alpha0', 'alpha1', 'gamma',
'sigma_alpha0', 'sigma_alpha1', "sigma_delta", 'sigma_y', 'log_lik'),
open_progress = FALSE, chains = 2, iter = 2000,
warmup = 1000, thin = 1, cores = 2)
dd[dd$id %in% idtrain,]
fit <- stan(file = file.path(.libPaths()[1], "ltjmm", "stan", "ltjmm.stan"),
data = setup$data,
pars = c('beta', 'delta', 'alpha0', 'alpha1', 'gamma',
'sigma_alpha0', 'sigma_alpha1', "sigma_delta", 'sigma_y', 'log_lik'),
open_progress = FALSE, chains = 2, iter = 2000,
warmup = 1000, thin = 1, cores = 2)
load("/Users/MonicaW/Documents/GitHub/LWPR/try_ltjmm.RData")
pairs()
=======
X.interaction.list[[m]][[k]] = X.list[[m]][,i]*X.list[[m]][,j]
}
}
}
X.interaction.list[[m]] = do.call(cbind, X.interaction.list[[m]])
#  X.plus.inter.list[[m]] = cbind(X.list[[m]], X.interaction.list[[m]])
}
X1.interaction.mean = apply(X.interaction.list[[1]], 2, mean)
X1.interaction.sd = apply(X.interaction.list[[1]], 2, sd)
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.mean)
X.interaction.list[[1]] = sweep(X.interaction.list[[1]], 2, X1.interaction.sd, "/")
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.mean)
X.interaction.list[[2]] = sweep(X.interaction.list[[2]], 2, X1.interaction.sd, "/")
# # if the std is really small just subtract the mean in the following step
# X1.interaction.sd = sapply(X1.interaction.sd, function(x) ifelse(x<1e-5, 1, x))
# X.interaction.list[[1]] = t(apply(X.interaction.list[[1]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
# X.interaction.list[[2]] = t(apply(X.interaction.list[[2]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
X.plus.inter.list = lapply(1:2, function(x) cbind(X.list[[x]], X.interaction.list[[x]]))
# --- computing distance correlation to select favorite number of features including interaction features ---
# number of features to be selected
p_dc = 200
cl = makeCluster(4) # number of cores you can use
registerDoParallel(cl)
dc.vec = foreach(col_ix = 1:ncol(X.plus.inter.list[[1]]), .packages = "energy", .combine = "c") %dopar% {
dcor(Y.list[[1]], X.plus.inter.list[[1]][,col_ix])
}
stopCluster(cl)
# dc.vec = apply(X.plus.inter.list[[1]], 2, function(feature) dcor(Y.list[[1]], feature))
print("Finish calculation distance correlation")
X.selected.feature.id = order(dc.vec, decreasing = TRUE)[1:p_dc]
X.selected.feature.list = lapply(1:2, function(ix) X.plus.inter.list[[ix]][, X.selected.feature.id])
# add to the last... probably to fix a random forest bug
feature.ncol = ncol(X.selected.feature.list[[1]])
colnames(X.selected.feature.list[[1]]) = as.character(seq(1, feature.ncol))
colnames(X.selected.feature.list[[2]]) = as.character(seq(1, feature.ncol))
# ----------------------------------------- main -------------------------------------------
# ------------- parameters --------------
nfolds.log = 5 # ordinal logistic: Sl
nfolds.llr = 5 # local linear regression
alpha0 = 0
gamma.vec = exp(rev(seq(-2, 7, length.out = 50)))
# initial point for optimization, first K-1 parameters are thetas, then the first coming p are coefficients the last p are slack variable
initial.x = c(seq(-2, 2, length.out = length(levels(label.list[[1]]))-1), rep(0,p_dc), rep(1,p_dc))
measure.type = "corr"
# ------------------------ ordinal logistic: Sl --------------------------
ordinlog.list = cv.ordinlog.en(label.list[[1]], X.selected.feature.list[[1]], Y.list[[1]], gamma.vec, alpha0, initial.x, nfolds.log, "corr")
ordin.ml = ordinlog.list$ordin.ml
sl.list = lapply(1:2, function(x) as.vector(X.selected.feature.list[[x]]%*%ordin.ml$w))
# delete outliers
sl.list = lapply(1:2, function(ix){
sl.list[[ix]][sl.list[[ix]] > boxplot(sl.list[[ix]], plot = F)$stats[5,1]] = boxplot(sl.list[[ix]], plot = F)$stats[5,1]
sl.list[[ix]][sl.list[[ix]] < boxplot(sl.list[[ix]], plot = F)$stats[1,1]] = boxplot(sl.list[[ix]], plot = F)$stats[1,1]
sl.list[[ix]]
})
# tuning parameter Di for SlRf
Di.vec = seq(sd(sl.list[[1]])/5, sd(sl.list[[1]])*2, length.out = 20)
# ----------------------------- SlRf LWPR ---------------------------------
par.list = cv.bothPen.noDb.nolambda(label.list[[1]], X.selected.feature.list[[1]], Y.list[[1]], alpha, nfolds.llr, sl.list[[1]], Di.vec)
source('~/GitHub/LWPR/function/fun.rfguided.R', echo=TRUE)
source('~/GitHub/LWPR/function/ordinlog1.R', echo=TRUE)
source('~/GitHub/LWPR/function/fun.noDb.R', echo=TRUE)
# ----------------------------- SlRf LWPR ---------------------------------
par.list = cv.bothPen.noDb.nolambda(label.list[[1]], X.selected.feature.list[[1]], Y.list[[1]], alpha, nfolds.llr, sl.list[[1]], Di.vec)
par.list
Di.selected = par.list$Di
lam.id = par.list$lambda.id
id.which = par.list$id.which
if(id.which == 1){
ml.rf = randomForest(x = X.selected.feature.list[[1]], y = Y.list[[1]], keep.inbag = T, ntree = 100)
wrf.list = rf.weight(ml.rf, X.selected.feature.list[[1]], X.selected.feature.list[[2]])
mymethod.res = SlRf.weight.noDb(wrf.list, Y.list[[1]], sl.list[[1]], sl.list[[2]], Di.selected)
}else{
mymethod.res = slnp.noDb(X.selected.feature.list[[1]], Y.list[[1]], sl.list[[1]], X.selected.feature.list[[2]], Y.list[[2]], sl.list[[2]], Di.selected)
}
Yhat.mymethod = mymethod.res$Yhat
rwrf.list = mymethod.res$w.list
Yhat.mymethodPen = predict.penalized.origin.method.nolambda(X.selected.feature.list[[1]], Y.list[[1]], X.selected.feature.list[[2]], rwrf.list, lam.id, alpha)
mae.mymethod = mean(abs(Yhat.mymethod - exp(Y.list[[2]])))
corr.mymethod = cor(Yhat.mymethod, exp(Y.list[[2]]))
mae.mymethodPen = mean(abs(Yhat.mymethodPen - exp(Y.list[[2]])))
t
a
corr.mymethod = cor(Yhat.mymethod, exp(Y.list[[2]]))
corr.mymethodPen = cor(Yhat.mymethodPen, exp(Y.list[[2]]))
Y.list[[1]]
Y.list[[2]]
exp(Y.list[[2]])
Yhat.mymethod
# ------------------------------ Caculating results ----------------------------------------
# -------------------- mae and corr --------------------------
Yhat.mymethod = exp(Yhat.mymethod)
Yhat.mymethodPen = exp(Yhat.mymethodPen)
mae.mymethod = mean(abs(Yhat.mymethod - exp(Y.list[[2]])))
corr.mymethod = cor(Yhat.mymethod, exp(Y.list[[2]]))
mae.mymethodPen = mean(abs(Yhat.mymethodPen - exp(Y.list[[2]])))
corr.mymethodPen = cor(Yhat.mymethodPen, exp(Y.list[[2]]))
source('~/GitHub/LWPR/function/fun.rfguided.R', echo=TRUE)
Yhat.mymethodPen = predict.penalized.origin.method.nolambda(X.selected.feature.list[[1]], Y.list[[1]], X.selected.feature.list[[2]], rwrf.list, lam.id, alpha)
predict.penalized.origin.method.nolambda = function(X.train, Y.train, X.test, wrf.list, lam.id, alpha){
n.test = dim(X.test)[1]
p = dim(X.test)[2]
nlambda = 100
#diff.mtx.list = diff.matrix(X.train, X.test)
diff.mtx.list = diff.matrix.simple(X.train, X.test)
#diff.mtx.list = diff.matrix.weight.standardize(X.train, X.test, wrf.list)
beta = c()
for (x in 1:n.test){
if(sd(Y.train[wrf.list[[x]]>1e-5])==0 | length(Y.train[wrf.list[[x]]>1e-5])<=10){
print(1)
beta[x] = sum(Y.train*wrf.list[[x]])
}
else{
print(2)
lam_max = lambda.max(diff.mtx.list[[x]], Y.train, wrf.list[[x]], alpha)
lambda.vec = exp(seq(log(lam_max), log(lam_max*1e-3), length.out = nlambda))
fit = glmnet(x = diff.mtx.list[[x]], y = Y.train, weights = wrf.list[[x]], alpha = alpha, lambda = lambda.vec, standardize = F)
# print(lambda.vec[lam.id])
print(as.matrix(coef(fit, s=lambda.vec)))
beta.x = as.vector(coef(fit, s=lambda.vec[lam.id]))
beta[x] = beta.x[1]}
}
return(beta)
}
Yhat.mymethodPen = predict.penalized.origin.method.nolambda(X.selected.feature.list[[1]], Y.list[[1]], X.selected.feature.list[[2]], rwrf.list, lam.id, alpha)
predict.penalized.origin.method.nolambda = function(X.train, Y.train, X.test, wrf.list, lam.id, alpha){
n.test = dim(X.test)[1]
p = dim(X.test)[2]
nlambda = 100
#diff.mtx.list = diff.matrix(X.train, X.test)
diff.mtx.list = diff.matrix.simple(X.train, X.test)
#diff.mtx.list = diff.matrix.weight.standardize(X.train, X.test, wrf.list)
beta = c()
for (x in 1:n.test){
if(sd(Y.train[wrf.list[[x]]>1e-5])==0 | length(Y.train[wrf.list[[x]]>1e-5])<=10){
print(1)
beta[x] = sum(Y.train*wrf.list[[x]])
}
else{
print(2)
lam_max = lambda.max(diff.mtx.list[[x]], Y.train, wrf.list[[x]], alpha)
lambda.vec = exp(seq(log(lam_max), log(lam_max*1e-3), length.out = nlambda))
fit = glmnet(x = diff.mtx.list[[x]], y = Y.train, weights = wrf.list[[x]], alpha = alpha, lambda = lambda.vec, standardize = F)
# print(lambda.vec[lam.id])
print(as.matrix(coef(fit, s=lambda.vec))[1,])
beta.x = as.vector(coef(fit, s=lambda.vec[lam.id]))
beta[x] = beta.x[1]}
}
return(beta)
}
Yhat.mymethodPen = predict.penalized.origin.method.nolambda(X.selected.feature.list[[1]], Y.list[[1]], X.selected.feature.list[[2]], rwrf.list, lam.id, alpha)
alpha
>>>>>>> 577b3e66b09e8ca9881c6553527d0abbffb58370
