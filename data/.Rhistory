load("C:/Users/peiyao/Desktop/ltjmm_PCA2.RData")
library(rstan)
funnel <- stan_demo("funnel", seed = 12345)   # has 5 divergent transitions
library(rstan)
funnel <- stan_demo("funnel", seed = 12345)   # has 5 divergent transitions
install.packages("rtools")
library(rtools)
install.packages(c("broom", "devtools", "dplyr", "ggplot2", "lava", "stringi"))
install.packages(c("broom", "devtools", "dplyr", "ggplot2", "lava", "stringi"))
install.packages(c("broom", "devtools", "dplyr", "ggplot2", "lava", "stringi"))
install.packages(c("broom", "devtools", "dplyr", "ggplot2", "lava", "stringi"))
install.packages(c("broom", "devtools", "dplyr", "ggplot2", "lava", "stringi"))
install.packages(c("broom", "devtools", "dplyr", "ggplot2", "lava", "stringi"))
library(rstan)
library(rstan)
library(rstan)
options(mc.cores = parallel::detectCores()).
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
funnel <- stan_demo("funnel", seed = 12345)   # has 5 divergent transitions
install.packages("Rtools")
install.packages("Rtools", dependencies = FALSE)
funnel <- stan_demo("funnel", seed = 12345)   # has 5 divergent transitions
load("C:/Users/peiyao/Desktop/ltjmm_PCA2.RData")
fit@model_pars
pairs(fit, pars = c("beta", "energy__"))
pairs(fit, pars = c("beta", "lp__"))
pairs(fit, pars = c("gamma", "lp__"))
pairs(fit, pars = c("beta[1]", "lp__"))
pairs(fit, pars = c("sigma_y", "lp__"))
library(rstan)
funnel <- stan_demo("funnel", seed = 12345)   # has 5 divergent transitions
library(caret)
library(methods) # "is function issue by Rscript"
library(energy)
library(glmnet)
library(randomForest)
library(foreach)
library(doParallel)
library(caret)
library(methods) # "is function issue by Rscript"
library(energy)
library(glmnet)
library(randomForest)
library(foreach)
library(doParallel)
source('~/GitHub/LWPR/function/fun.noDb.R')
source('~/GitHub/LWPR/function/fun.rfguided.R')
source('~/GitHub/LWPR/function/ordinlog1.R')
t = 2
a = 1
library(readr)
getwd()
setwd("C:/Users/peiyao/Documents/GitHub/LWPR/data")
X0 = as.matrix(read_table("X1a.txt", col_names = F))
Y0 = as.matrix(read_table("Y5T.txt", col_names = F))[, 4+5*(t-1)]
label0 = as.ordered(read_table("label1.txt", col_names = F)$X1)
# remove NAs in Y
id.cut1 = !is.na(Y0)
X0 = X0[id.cut1, ]
Y0 = Y0[id.cut1]
label0 = label0[id.cut1]
# remove negatives in Y
id.cut2 = (Y0 > 0)
X0 = X0[id.cut2,]
Y0 = Y0[id.cut2]
label0 = label0[id.cut2]
# only select MRI+PET
Xs = X0[, 1:186]
# within group median , each element from the list is the median within the group
# list of length 3, each element 1 by 186 group median of each feature
Xs.med.list = lapply(levels(label0), function(x) apply(Xs[label0 == x, ], 2, function(y) median(y, na.rm = T)))
# impute with median
Xs.missing = is.na(Xs)
for (i in 1:nrow(Xs)){
ip.med = Xs.med.list[[as.numeric(label0[i])]]
Xs[i, Xs.missing[i, ]] = ip.med[Xs.missing[i, ]]
}
X = Xs
Y = Y0
label = label0
myseed = 7623
idtrain = unlist(createDataPartition(label, times = 1, p = 3/4))
idtest = (1:n)[-idtrain]
Y = log(Y+1)
id = list(idtrain, idtest)
Y.list = lapply(1:2, function(x) Y[id[[x]]]) # train,test
X.list = lapply(1:2, function(x) X[id[[x]],])
label.list = lapply(1:2, function(x) label[id[[x]]])
# ---------------------- creating training and testing set -----------------------------
n = dim(X)[1]
p = dim(X)[2]
set.seed(myseed)
idtrain = unlist(createDataPartition(label, times = 1, p = 3/4))
idtest = (1:n)[-idtrain]
Y = log(Y+1)
id = list(idtrain, idtest)
Y.list = lapply(1:2, function(x) Y[id[[x]]]) # train,test
X.list = lapply(1:2, function(x) X[id[[x]],])
label.list = lapply(1:2, function(x) label[id[[x]]])
# ----------------------------------- do scale ----------------------------------
X1.mean = apply(X.list[[1]], 2, mean)
X1.sd = apply(X.list[[1]], 2, sd)
# if the std is really small just subtract the mean in the following step
X1.sd = sapply(X1.sd, function(x) ifelse(x<1e-5, 1, x))
X.list[[1]] = t(apply(X.list[[1]], 1, function(x) (x - X1.mean)/X1.sd))
X.list[[2]] = t(apply(X.list[[2]], 1, function(x) (x - X1.mean)/X1.sd))
print("Finish scaling features")
# -------------------------------------------------------------------------------
# ----------------------------- Calculating interaction term ------------------------------
X.interaction.list = list()
X.plus.inter.list = list()
for (m in 1:2){
X.interaction.list[[m]] = list()
k = 0
for (i in 1:p){
for (j in 1:p){
if (i < j){
k = k+1
X.interaction.list[[m]][[k]] = X.list[[m]][,i]*X.list[[m]][,j]
}
}
}
X.interaction.list[[m]] = do.call(cbind, X.interaction.list[[m]])
#  X.plus.inter.list[[m]] = cbind(X.list[[m]], X.interaction.list[[m]])
}
X1.interaction.mean = apply(X.interaction.list[[1]], 2, mean)
X1.interaction.sd = apply(X.interaction.list[[1]], 2, sd)
# if the std is really small just subtract the mean in the following step
X1.interaction.sd = sapply(X1.interaction.sd, function(x) ifelse(x<1e-5, 1, x))
X.interaction.list[[1]] = t(apply(X.interaction.list[[1]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
X.interaction.list[[2]] = t(apply(X.interaction.list[[2]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
X.plus.inter.list = lapply(1:2, function(x) cbind(X.list[[x]], X.interaction.list[[x]]))
# --- computing distance correlation to select favorite number of features including interaction features ---
# number of features to be selected
p_dc = 200
cl = makeCluster(4) # number of cores you can use
registerDoParallel(cl)
myseed
# ---------------------- creating training and testing set -----------------------------
n = dim(X)[1]
p = dim(X)[2]
set.seed(myseed)
idtrain = unlist(createDataPartition(label, times = 1, p = 3/4))
idtest = (1:n)[-idtrain]
Y = log(Y+1)
id = list(idtrain, idtest)
Y.list = lapply(1:2, function(x) Y[id[[x]]]) # train,test
X.list = lapply(1:2, function(x) X[id[[x]],])
label.list = lapply(1:2, function(x) label[id[[x]]])
# ----------------------------------- do scale ----------------------------------
X1.mean = apply(X.list[[1]], 2, mean)
X1.sd = apply(X.list[[1]], 2, sd)
# if the std is really small just subtract the mean in the following step
X1.sd = sapply(X1.sd, function(x) ifelse(x<1e-5, 1, x))
X.list[[1]] = t(apply(X.list[[1]], 1, function(x) (x - X1.mean)/X1.sd))
X.list[[2]] = t(apply(X.list[[2]], 1, function(x) (x - X1.mean)/X1.sd))
print("Finish scaling features")
# ----------------------------- Calculating interaction term ------------------------------
X.interaction.list = list()
X.plus.inter.list = list()
for (m in 1:2){
X.interaction.list[[m]] = list()
k = 0
for (i in 1:p){
for (j in 1:p){
if (i < j){
k = k+1
X.interaction.list[[m]][[k]] = X.list[[m]][,i]*X.list[[m]][,j]
}
}
}
X.interaction.list[[m]] = do.call(cbind, X.interaction.list[[m]])
#  X.plus.inter.list[[m]] = cbind(X.list[[m]], X.interaction.list[[m]])
}
X1.interaction.mean = apply(X.interaction.list[[1]], 2, mean)
X1.interaction.sd = apply(X.interaction.list[[1]], 2, sd)
# if the std is really small just subtract the mean in the following step
X1.interaction.sd = sapply(X1.interaction.sd, function(x) ifelse(x<1e-5, 1, x))
X.interaction.list[[1]] = t(apply(X.interaction.list[[1]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
X.interaction.list[[2]] = t(apply(X.interaction.list[[2]], 1, function(x) (x - X1.interaction.mean)/X1.interaction.sd))
X.plus.inter.list = lapply(1:2, function(x) cbind(X.list[[x]], X.interaction.list[[x]]))
# --- computing distance correlation to select favorite number of features including interaction features ---
# number of features to be selected
p_dc = 200
cl = makeCluster(4) # number of cores you can use
